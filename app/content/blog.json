[
	{
		"slug": "vector-index",
		"date": "12/12/2024",
		"author": "Martin Loretz",
		"title": "Accelerate GPT Output Embedding computations with a Vector Index",
		"description": "The input and output embeddings of transformer-based LLMs are massive. For example, GPT-2's embeddings make up 38.5M (50257 x 768) out of 124M total parameters (~30%)."
	},
	{
		"slug": "mlp-activation-ratio",
		"date": "21/09/2024",
		"author": "Martin Loretz",
		"title": "Exploring the activation ratios of different MLP layers",
		"description": " I've become interested in understanding how many neurons in a layer (Linear + ReLU) actually contribute to the next layer versus those that remain inactive after the activation function."
	},
	{
		"slug": "polynomial-mlp",
		"date": "03/05/2024",
		"author": "Martin Loretz",
		"title": "Polynomials as weights and activations for MLP's",
		"description": "I recently read the Kolmogorov-Arnold Networks paper and I'm fascinated by the idea of moving from fixed activations on the nodes of a MLP to learned ones at the edges. Combined with some other thoughts, I applied this idea to image classification and in this process came up with the idea of polynomial MLP's."
	}
]
